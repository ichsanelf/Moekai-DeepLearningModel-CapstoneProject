# -*- coding: utf-8 -*-
"""capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CpqaxVjLMUm1tnvXgFerRKQgTO5k7R7F

# Import Library
"""

#!pip install tensorflowjs

#!pip install --upgrade pip
#!pip install tensorflow

# Commented out IPython magic to ensure Python compatibility.
# Mengimpor libraries umum yang sering digunakan
import os, shutil
import random
from random import sample
from shutil import copyfile
import pathlib
from pathlib import Path
import seaborn as sns
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq

# Mengimpor libraries untuk visualisasi
# %matplotlib inline
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
from matplotlib.image import imread

# Mengimpor libraries untuk pemrosesan data gambar
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise

# Mengimpor libraries untuk pembuatan dan evaluasi model
import keras

import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras import Model, layers, models
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Nadam
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, Input, Embedding, GlobalAveragePooling1D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.applications import ResNet50


# Mengabaikan peringatan
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

"""# Data Collecting"""

!kaggle datasets download -d ichsanelf/private-dataset
!unzip private-dataset.zip

# Define the base path
path = "Dataset/"

# Initialize the dictionary
human_emotion = {}

# Iterate through each subfolder and collect images
for folder in os.listdir(path):
    folder_path = os.path.join(path, folder)
    if os.path.isdir(folder_path):  # Check if it is a directory
        images = []
        for filename in os.listdir(folder_path):
            if filename.endswith(('.png', '.jpg', '.jpeg')):  # Filter image files
                images.append(os.path.join(folder_path, filename))
        human_emotion[folder] = images

# Print the dictionary to check the result
print(human_emotion.keys())

# Menampilkan secara acak 5 gambar di bawah setiap kelas dari data latih
fig, axs = plt.subplots(len(human_emotion.keys()), 5, figsize=(15, 15))

for i, class_name in enumerate(human_emotion.keys()):
    images = np.random.choice(human_emotion[class_name], 5, replace=False)

    for j, image_name in enumerate(images):
        img_path = human_emotion[class_name][j]
        img = Image.open(img_path)  # Konversi menjadi skala keabuan
        axs[i, j].imshow(img)
        axs[i, j].set(xlabel=class_name, xticks=[], yticks=[])


fig.tight_layout()

"""# Data Splitting"""

# Define source path
emotion_path = "Dataset/"

# Create a list that stores data for each filenames, filepaths, and labels in the data
file_name = []
labels = []
full_path = []

# Get data image filenames, filepaths, labels one by one with looping, and store them as dataframe
for path, subdirs, files in os.walk(emotion_path):
    for name in files:
        full_path.append(os.path.join(path, name))
        labels.append(path.split('/')[-1])
        file_name.append(name)

images_df = pd.DataFrame({"path":full_path,'file_name':file_name,"labels":labels})

# Plot the distribution of images across the classes
Label = images_df['labels']
plt.figure(figsize = (6,6))
sns.set_style("darkgrid")
plot_data = sns.countplot(Label)

# Variabel yang digunakan pada pemisahan data ini dimana variabel x = data path dan y = data labels
X= images_df['path']
y= images_df['labels']

# Split dataset awal menjadi data train dan test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=300)

# Menyatukan kedalam masing-masing dataframe
df_tr = pd.DataFrame({'path':X_train,'labels':y_train,'set':'train'})
df_te = pd.DataFrame({'path':X_test,'labels':y_test,'set':'test'})

# Print hasil diatas untuk melihat panjang size data training dan testing
print('train size', len(df_tr))
print('test size', len(df_te))

# Gabungkan DataFrame df_tr dan df_te
df_all = pd.concat([df_tr, df_te], ignore_index=True)

print('===================================================== \n')
print(df_all.groupby(['set', 'labels']).size(), '\n')
print('===================================================== \n')

# Cek sample data
print(df_all.sample(5))

# Memanggil dataset asli yang berisi keseluruhan data gambar yang sesuai dengan labelnya
datasource_path = "Dataset/"
# Membuat variabel Dataset, dimana nanti menampung data yang telah dilakukan pembagian data training dan testing
dataset_path = "Dataset-Final/"

for index, row in tq(df_all.iterrows()):
    # Deteksi filepath
    file_path = row['path']
    if os.path.exists(file_path) == False:
            file_path = os.path.join(datasource_path,row['labels'],row['image'].split('.')[0])

    # Buat direktori tujuan folder
    if os.path.exists(os.path.join(dataset_path,row['set'],row['labels'])) == False:
        os.makedirs(os.path.join(dataset_path,row['set'],row['labels']))

    # Tentukan tujuan file
    destination_file_name = file_path.split('/')[-1]
    file_dest = os.path.join(dataset_path,row['set'],row['labels'],destination_file_name)

    # Salin file dari sumber ke tujuan
    if os.path.exists(file_dest) == False:
        shutil.copy2(file_path,file_dest)

"""# Model"""

# Define training and test directories
TRAIN_DIR = "Dataset-Final/train/"
TEST_DIR = "Dataset-Final/test/"

train_game = {}
for folder in os.listdir(TRAIN_DIR):
    folder_path = os.path.join(TRAIN_DIR, folder)
    if os.path.isdir(folder_path):
        train_game[folder] = len(os.listdir(folder_path))

print(f"Total number of games images in training :", train_game)

test_game = {}
for folder in os.listdir(TEST_DIR):
    folder_path = os.path.join(TEST_DIR, folder)
    if os.path.isdir(folder_path):
        test_game[folder] = len(os.listdir(folder_path))

print(f"Total number of games images in test :", test_game)

datagen = ImageDataGenerator(
    rescale=1/255.,
    #zoom_range=0.2,
    #rotation_range=10,
    #width_shift_range=0.1,
    #height_shift_range=0.1,
    #shear_range=0.2,
    #horizontal_flip=True,
    fill_mode='nearest',
    )
test_datagen = ImageDataGenerator(rescale=1 / 255.)

train_generator = datagen.flow_from_directory(TRAIN_DIR,
                                              batch_size=16,
                                              target_size=(200, 200),
                                              color_mode="grayscale",
                                              class_mode='categorical',
                                              shuffle=True)

validation_generator = test_datagen.flow_from_directory(TEST_DIR,
                                                        batch_size=16,
                                                        target_size=(200, 200),
                                                        color_mode="grayscale",
                                                        class_mode='categorical',
                                                        shuffle=False)

tf.keras.backend.clear_session()
# Load the ResNet50 model without the top layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(200, 200, 3))

# Adjust for grayscale input
input_tensor = tf.keras.layers.Input(shape=(200, 200, 1))
x = tf.keras.layers.Conv2D(3, (3, 3), padding='same')(input_tensor)  # Convert to 3 channels
x = base_model(x)

n_classes=3
# Add custom top layers
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(n_classes, activation='softmax')(x)

model = tf.keras.Model(inputs=input_tensor, outputs=x)
optimize_lr = Nadam(learning_rate=0.0001)

# Compile the model
model.compile(optimizer=optimize_lr, loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

folder_weights = {}

# Calculate the total number of images across all folders
total_classes = len(train_game)
total_images = sum(train_game.values())

# Iterate through each folder and calculate the weight
for folder, count in train_game.items():
    weight = (1 / count) * total_images / total_classes
    folder_weights[folder] = weight

# Print the calculated weights
for folder, weight in folder_weights.items():
    print(f"Weight for {folder}: {weight}")

"""# Evaluasi

## Epoch process
"""

# Commented out IPython magic to ensure Python compatibility.
early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.0000000001)

# %time

# Fitting / training model
history_1 = model.fit(train_generator,
                        epochs=50,
                        batch_size=16,
                        validation_data=validation_generator,
                        class_weight = folder_weights,
                        callbacks=[early_stopping, reduce_lr]
                        )

"""## Val acc & loss Plot"""

acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']
loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r')
plt.plot(epochs, val_acc, 'b')
plt.title('Training and Validation Accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(epochs, loss, 'r')
plt.plot(epochs, val_loss, 'b')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.title('Training and Validation Loss')
plt.show()

"""## Confusion Matrix"""

validation_generator.reset()

preds_1 = model.predict(validation_generator,verbose=0)

predicted_labels = np.argmax(preds_1, axis=1)

cm = pd.DataFrame(data=confusion_matrix(validation_generator.classes, predicted_labels),
                  index=train_generator.class_indices,
                  columns=train_generator.class_indices)

sns.heatmap(cm,annot=True,fmt="d")

print("\n")
print(classification_report(y_true=validation_generator.classes,
                            y_pred=predicted_labels,
                            target_names = train_generator.class_indices,
                            digits=4))

"""# Konversi Model

tf.keras.backend.clear_session()
## SavedModel
"""

#tf.keras.backend.clear_session()

#save_path = 'mymodel/'
#tf.saved_model.save(model, save_path)

"""## Tensorflow.js"""

#!tensorflowjs_converter --input_format=tf_saved_model --output_format=tfjs_graph_model --quantize_uint8 /content/mymodel /content/tfjs_model

# prompt: download tfjs_model folder

#!zip -r /content/savedmodel.zip /content/mymodel
#from google.colab import files
#files.download("/content/savedmodel.zip")

"""# Inference"""

from google.colab import files
uploaded = files.upload()

# Path to your saved model (adjust as necessary)
model_path = '/content/mymodel/'

# Load the model
model = tf.saved_model.load(model_path)

# Prepare input data
img_path = list(uploaded.keys())[0]
img = image.load_img(img_path, target_size=(150, 150), color_mode="grayscale")
input_arr = image.img_to_array(img)
input_arr = np.array([input_arr])
input_arr = input_arr / 255.
infer = model.signatures["serving_default"]
predictions = infer(tf.constant(input_arr))['output_0']

predictions_array = predictions.numpy()
predicted_class = np.argmax(predictions_array)

class_indices_ = validation_generator.class_indices
index_to_class = {v: k for k, v in class_indices_.items()}

predicted_class_name = index_to_class[predicted_class]

plt.figure(figsize=(6, 6))
plt.axis('off')
out = f"I am {max(predictions_array[0]) * 100:.2f}% confident that this is a {predicted_class_name} case"
plt.title("Prediksi Ekspresi :\n" + out)
plt.imshow(np.squeeze(input_arr))  # Ensuring the image is displayed in grayscale
plt.show()

!pip freeze > requirements.txt

# Read the file and filter the lines
with open('requirements.txt', 'r') as f:
    lines = f.readlines()

# Define the packages you want to keep
required_packages = ['pandas', 'matplotlib', 'PIL', 'skimage', 'random',
                     'keras', 'tensorflow', 'tqdm', 'numpy', 'seaborn', 'pathlib'
                    'sklearn', 'os', 'cv2', 'shutil', 'random', 'pathlib']

# Filter the lines
filtered_lines = [line for line in lines if any(package in line for package in required_packages)]

# Write the filtered lines to a new requirements file
with open('requirements.txt', 'w') as f:
    f.writelines(filtered_lines)

# Display the contents of the new requirements file
!cat requirements.txt